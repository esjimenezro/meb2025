{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicando K-Means Mediante el Algoritmo de Maximización de la Esperanza\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/e/e5/KMeans-Gaussian-data.svg\" width=\"500px\" height=\"300px\" />\n",
    "\n",
    "> Ya derivamos la forma general del algoritmo de maximización de la esperanza, e hicimos un ejemplo sencillo. Es hora de ver como se aplica este algoritmo a ejemplos más interesantes.\n",
    "\n",
    "> Ahora, apliquémoslo para explicar el algoritmo K-Means.\n",
    "\n",
    "> **Objetivos:**\n",
    "> - Explicar el algoritmo K-Means usando el algoritmo de maximización de la esperanza.\n",
    "\n",
    "> **Referencias:**\n",
    "> - Bayesian Methods for Machine Learning course, HSE University, Coursera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recordando el algoritmo K-Means\n",
    "\n",
    "**Problema:** dado un conjunto de observaciones $x_1, x_2, \\dots, x_N \\in \\mathbb{R}^d$, se debe particionar las $N$ observaciones en $k$ ($\\leq N$) clusters $\\{1, 2, \\dots, k\\}$, de manera que se minimice la suma de distancias al cuadrado (varianza)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos función para generar datos\n",
    "\n",
    "# Importamos pyplot\n",
    "\n",
    "# Importamos numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grupos \"reales\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algoritmo:**\n",
    "\n",
    "1. Inicializar los parámetros $\\theta = \\{\\mu_1, \\dots, \\mu_k\\}$ de manera aleatoria.\n",
    "2. Repetir hasta la convergencia (hasta que los parámetros no varíen):\n",
    "   1. Para cada punto calcule el centroide más cercano:\n",
    "      $$\n",
    "      c_i = \\arg \\min_{c} ||x_i - \\mu_c||.\n",
    "      $$\n",
    "      \n",
    "   2. Actualizar centroides:\n",
    "      $$\n",
    "      \\mu_c = \\frac{\\sum_{i: c_i = c} x_i}{\\sum_{i: c_i = c} 1}\n",
    "      $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conexión con algoritmo de maximización de la esperanza\n",
    "\n",
    "Desde una perspectiva probabilística, modelamos el K-Means como un modelo de variable latente:\n",
    "\n",
    "![latent](figures/latent_model.png)\n",
    "\n",
    "En efecto, la idea es similar al modelo de mezclas Gaussianas, fijando:\n",
    "\n",
    "- $\\Sigma_c = I$.\n",
    "- $\\pi_c = \\frac{1}{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo anterior, la distribución:\n",
    "\n",
    "$$\n",
    "p(x_i | t_i=c, \\theta) = \\mathcal{N}(x_i | \\mu_c, I) = \\frac{1}{\\sqrt{(2 \\pi)^n}} \\exp\\left(-\\frac{1}{2}||x_i - \\mu_c||^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-step\n",
    "\n",
    "Para cada punto, debemos encontrar la distribución variacional que minimiza:\n",
    "\n",
    "$$\n",
    "q^{j+1} = \\arg \\min_{q \\in Q}\\sum_{i=1}^{N} \\mathcal{KL}(q(t_i) || p(t_i|x_i, \\theta^j)).\n",
    "$$\n",
    "\n",
    "Sin embargo, recordamos que el K-Means es un esquema de hard clustering, que solo acepta que un punto pertenezca a un único cluster.\n",
    "\n",
    "Es decir las posibles opciones para $q$ es el conjunto $Q$ de funciones $\\delta$:\n",
    "\n",
    "$$\n",
    "Q = \\left\\{f: f(t)=\\delta(t - c) \\right\\},\n",
    "$$\n",
    "\n",
    "donde \n",
    "\n",
    "$$\n",
    "\\delta(t - c) = \\left\\lbrace\\begin{array}{ccc} \n",
    "                1 & \\text{si} & t=c\\\\\n",
    "                0 & \\text{en otro caso}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_delta(t, c):\n",
    "    return (t == c) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(8, 12, 101)\n",
    "y = fun_delta(t, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, y, 'ro', ms=8)\n",
    "plt.vlines(t, 0, y, colors='r', lw=4)\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** \n",
    "\n",
    "Supongamos que tenemos la siguiente distribución posterior $p(t|x, \\theta)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1, 2, 3], [0.15, 0.25, 0.6], 'ro', ms=8, label=r\"Distribución posterior $p(t|x, \\theta)$\")\n",
    "plt.vlines([1, 2, 3], 0, [0.15, 0.25, 0.6], colors='r', lw=4)\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axhline(y=1, ls='--', color='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál será la función tipo $\\delta$ que más se acerque a esta distribución (de acuerdo a la divergencia $\\mathcal{KL}$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 3 opciones razonables para $q$:\n",
    "\n",
    "1. $q=\\delta(t - 1)$\n",
    "\n",
    "   \\begin{align}\n",
    "   \\mathcal{KL}(q(t) || p(t|x, \\theta)) & = \\mathbb{E}_{q}\\left[\\log \\frac{q(t)}{p(t|x, \\theta)}\\right] \\\\\n",
    "                                        & = \\sum_{c=1}^{3} q(t=c)\\log \\frac{q(t=c)}{p(t=c|x, \\theta)} \\\\\n",
    "                                        & = 1 \\log \\frac{1}{0.15} + 0 + 0 \\approx 1.8971\n",
    "   \\end{align}\n",
    "\n",
    "2. $q=\\delta(t - 2)$\n",
    "\n",
    "   \\begin{align}\n",
    "   \\mathcal{KL}(q(t) || p(t|x, \\theta)) & = \\mathbb{E}_{q}\\left[\\log \\frac{q(t)}{p(t|x, \\theta)}\\right] \\\\\n",
    "                                        & = \\sum_{c=1}^{3} q(t=c)\\log \\frac{q(t=c)}{p(t=c|x, \\theta)} \\\\\n",
    "                                        & = 0 + 1 \\log \\frac{1}{0.25} + 0 \\approx 1.3863\n",
    "   \\end{align}\n",
    "\n",
    "3. $q=\\delta(t - 3)$\n",
    "\n",
    "   \\begin{align}\n",
    "   \\mathcal{KL}(q(t) || p(t|x, \\theta)) & = \\mathbb{E}_{q}\\left[\\log \\frac{q(t)}{p(t|x, \\theta)}\\right] \\\\\n",
    "                                        & = \\sum_{c=1}^{3} q(t=c)\\log \\frac{q(t=c)}{p(t=c|x, \\theta)} \\\\\n",
    "                                        & = 0 + 0 + 1 \\log \\frac{1}{0.6} \\approx 0.5108\n",
    "   \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De modo que, como se esperaba, la distribución $\\delta$ más cercana a la posterior es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1, 2, 3], [0.15, 0.25, 0.6], 'ro', ms=8, label=r\"Distribución posterior $p(t|x, \\theta)$\")\n",
    "plt.plot([1, 2, 3], [0, 0, 1], 'bo', ms=5, label=r\"Distribución variacional $q(t)$ más cercana\")\n",
    "plt.vlines([1, 2, 3], 0, [0.15, 0.25, 0.6], colors='r', lw=4)\n",
    "plt.vlines([1, 2, 3], 0, [0, 0, 1], colors='b', lw=2)\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axhline(y=1, ls='--', color='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, la distribución variacional pondrá toda la probabilidad en el valor que tenga la mayor probabilidad en la distribución posterior.\n",
    "\n",
    "Matemáticamente:\n",
    "\n",
    "$$\n",
    "q^{j+1}(t_i) = \\delta(t_i - c_i^{j+1}),\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$\n",
    "c_i^{j+1} = \\arg \\max_c p(t_i=c | x_i, \\theta^j).\n",
    "$$\n",
    "\n",
    "Por otra parte, recordemos que la distribución posterior:\n",
    "\n",
    "\\begin{align}\n",
    "p(t_i=c | x_i, \\theta^j) & = \\frac{p(x_i | t_i=c, \\theta) p(t_i=c | \\theta)}{p(x_i | \\theta)} \\\\\n",
    "                         & = \\frac{1}{Z} \\exp\\left(-\\frac{1}{2}||x_i - \\mu_c||^2\\right) \\pi_c \\\\\n",
    "                         & = \\frac{1}{kZ} \\exp\\left(-\\frac{1}{2}||x_i - \\mu_c||^2\\right).\n",
    "\\end{align}\n",
    "\n",
    "De modo que:\n",
    "\n",
    "$$\n",
    "c_i^{j+1} = \\arg \\max_c p(t_i=c | x_i, \\theta^j) = \\arg \\min_c ||x_i - \\mu_c||^2\n",
    "$$\n",
    "\n",
    "#### ¡Justo como en K-Means!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos actualizar los parámetros $\\mu$ de modo que maximicen la verosimilitud conjunta (recordar que la matriz de covarianzas y los pesos están fijos):\n",
    "\n",
    "$$\n",
    "\\max_{\\mu} \\sum_{i=1}^{N} \\mathbb{E}_{q^{j+1}(t_i)} \\left[\\log p(x_i, t_i| \\mu)\\right].\n",
    "$$\n",
    "\n",
    "Reutilizando lo que habíamos encontrado para el modelo de mezclas Gaussianas en general:\n",
    "\n",
    "$$\n",
    "\\mu_c^{j+1} = \\frac{\\sum_{i=1}^{N} q^{j+1}(t_i=c)x_i}{\\sum_{i=1}^{N} q(t_i=c)},\n",
    "$$\n",
    "\n",
    "donde \n",
    "\n",
    "$$\n",
    "q^{j+1}(t_i) = \\delta(t_i - c_i^{j+1}).\n",
    "$$\n",
    "\n",
    "De esta manera:\n",
    "\n",
    "$$\n",
    "\\mu_c^{j+1} = \\frac{\\sum_{i: c_i^{j+1}=c} x_i}{\\sum_{i: c_i^{j+1}=c} 1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¡Justo como en K-Means!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. En resumen\n",
    "\n",
    "K-Means es un caso particular del algoritmo de maximización de la esperanza para mezclas Gaussianas:\n",
    "\n",
    "- $\\Sigma_c = I$.\n",
    "- $\\pi_c = \\frac{1}{k}$.\n",
    "- Se consideran distribuciones variacionales tipo $\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
