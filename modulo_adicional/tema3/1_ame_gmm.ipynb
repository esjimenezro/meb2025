{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de maximización de la esperanza para el modelo de mezclas Gaussianas\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Gauss-sum.svg/800px-Gauss-sum.svg.png\" width=\"500px\" height=\"300px\" />\n",
    "\n",
    "\n",
    "> Ya derivamos la forma general del algoritmo de maximización de la esperanza, e hicimos un ejemplo sencillo. Es hora de ver como se aplica este algoritmo a ejemplos más interesantes.\n",
    "\n",
    "> Comenzaremos por aplicarlo al problema de mezclas Gaussianas.\n",
    "\n",
    "> **Objetivos:**\n",
    "> - Aplicar el algoritmo de maximización de la esperanza al modelo de mezclas Gaussianas.\n",
    "\n",
    "> **Referencias:**\n",
    "> - Bayesian Methods for Machine Learning course, HSE University, Coursera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recordamos el problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos función para generar datos\n",
    "\n",
    "# Importamos scipy.stats.multivariate_normal\n",
    "\n",
    "# Importamos pyplot\n",
    "\n",
    "# Importamos numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos parámetros\n",
    "\n",
    "# Definimos VA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n",
    "plt.scatter(data[\"income\"], data[\"debt\"], c=\"gray\", alpha=0.6)\n",
    "\n",
    "# Gaussiana 1\n",
    "x = np.linspace(0, 6, 100)\n",
    "y = np.linspace(0, 5, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X1.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "\n",
    "# Gaussiana 2\n",
    "x = np.linspace(0, 4, 100)\n",
    "y = np.linspace(4, 9, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X2.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "\n",
    "# Gaussiana 3\n",
    "x = np.linspace(3, 8, 100)\n",
    "y = np.linspace(2, 8, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X3.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "\n",
    "plt.xlabel(\"Ingresos mensuales (x100k MXN)\")\n",
    "plt.ylabel(\"Deuda (x100k MXN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordamos que en un **modelo de mezclas Gaussianas** tenemos un conjunto de puntos el cual queremos modelar como una mezcla de Gaussianas:\n",
    "\n",
    "$$\n",
    "p(x | \\theta) = \\sum_{c=1}^{k} \\pi_c \\mathcal{N}(x | \\mu_c, \\Sigma_c).\n",
    "$$\n",
    "\n",
    "Los parámetros de este modelo son:\n",
    "\n",
    "$$\n",
    "\\theta = \\left\\{(\\pi_c, \\mu_c, \\Sigma_c): c=1,\\dots, k \\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde una perspectiva probabilística, esto lo podemos modelar como un modelo de variable latente:\n",
    "\n",
    "![latent](figures/latent_model.png)\n",
    "\n",
    "con las siguientes distribuciones:\n",
    "\n",
    "- $p(t_i=c | \\theta) = \\pi_c$\n",
    "\n",
    "- $p(x_i | t_i=c, \\theta) = \\mathcal{N}(x_i | \\mu_c, \\Sigma_c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conexión con algoritmo de maximización de la esperanza\n",
    "\n",
    "Obtengamos las expresiones detalladas en los pasos del algoritmo de maximización de la esperanza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-step\n",
    "\n",
    "Para cada punto, debemos calcular la distribución posterior:\n",
    "\n",
    "$$\n",
    "q(t_i) = p(t_i|x_i, \\theta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, usando el teorema de Bayes:\n",
    "\n",
    "\\begin{align}\n",
    "p(t_i=c | x_i, \\theta) & = \\frac{p(x_i | t_i=c, \\theta) p(t_i=c | \\theta)}{p(x_i | \\theta)} \\\\\n",
    "                       & = \\frac{\\mathcal{N}(x_i | \\mu_c, \\Sigma_c) \\pi_c}{\\sum_{c=1}^{k} \\pi_c \\mathcal{N}(x | \\mu_c, \\Sigma_c)}. \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-step\n",
    "\n",
    "Debemos actualizar los parámetros de modo que maximicen la verosimilitud conjunta:\n",
    "\n",
    "$$\n",
    "\\max_{\\theta} \\sum_{i=1}^{N} \\mathbb{E}_{q(t_i)} \\left[\\log p(x_i, t_i| \\theta)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desarrollemos un poco la función a maximizar:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{N} \\mathbb{E}_{q(t_i)} \\left[\\log p(x_i, t_i| \\theta)\\right] \n",
    "& = \\sum_{i=1}^{N} \\sum_{c=1}^{k} q(t_i=c) \\log \\left[p(x_i| t_i=c \\theta) p(t_i=c | \\theta)\\right] \\\\\n",
    "& = \\sum_{i=1}^{N} \\sum_{c=1}^{k} q(t_i=c) \\log \\left[\\mathcal{N}(x_i | \\mu_c, \\Sigma_c) \\pi_c\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar el entendimiento, resolvamos el caso escalar:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^{N} \\mathbb{E}_{q(t_i)} \\left[\\log p(x_i, t_i| \\theta)\\right] \n",
    "& = \\sum_{i=1}^{N} \\sum_{c=1}^{k} q(t_i=c) \\log \\left[p(x_i| t_i=c, \\theta) p(t_i=c | \\theta)\\right] \\\\\n",
    "& = \\sum_{i=1}^{N} \\sum_{c=1}^{k} q(t_i=c) \\log \\left[\\mathcal{N}(x_i | \\mu_c, \\sigma_c^2) \\pi_c\\right] \\\\\n",
    "& = \\sum_{i=1}^{N} \\sum_{c=1}^{k} q(t_i=c) \\log \\left[\\frac{\\pi_c}{\\sqrt{2 \\pi \\sigma_c^2}}\\exp\\left\\{-\\frac{(x_i - \\mu_c)^2}{2\\sigma_c^2}\\right\\}\\right] \\\\\n",
    "& = \\sum_{i=1}^{N} \\sum_{c=1}^{k} q(t_i=c) \\left[\\log \\pi_c - \\frac{1}{2}\\log 2 \\pi - \\frac{1}{2}\\log\\sigma_c^2 -\\frac{(x_i - \\mu_c)^2}{2\\sigma_c^2}\\right] := f(\\theta)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivamos respecto a $\\mu_c$ e igualamos a cero:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f(\\theta)}{\\partial \\mu_c} = \\sum_{i=1}^{N} q(t_i=c)\\frac{x_i - \\mu_c}{\\sigma_c^2} & = 0 \\\\\n",
    "\\Leftrightarrow \\sum_{i=1}^{N} q(t_i=c)\\left(x_i - \\mu_c\\right) & = 0 \\\\\n",
    "\\Leftrightarrow \\mu_c & = \\frac{\\sum_{i=1}^{N} q(t_i=c)x_i}{\\sum_{i=1}^{N} q(t_i=c)}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivamos respecto a $\\sigma_c^2$ e igualamos a cero:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f(\\theta)}{\\partial \\sigma_c^2} = \\sum_{i=1}^{N} q(t_i=c)\\left[- \\frac{1}{2\\sigma_c^2}+\\frac{(x_i - \\mu_c)^2}{2(\\sigma_c^2)^2}\\right] & = 0 \\\\\n",
    "\\Leftrightarrow \\sum_{i=1}^{N} q(t_i=c)\\left[-1+\\frac{(x_i - \\mu_c)^2}{\\sigma_c^2}\\right] & = 0 \\\\\n",
    "\\Leftrightarrow \\sigma_c^2 & = \\frac{\\sum_{i=1}^{N} q(t_i=c)(x_i - \\mu_c)^2}{\\sum_{i=1}^{N} q(t_i=c)}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de los parámetros $\\pi_c$, tenemos que tener en cuenta la restricción $\\sum_{c=1}^{k} \\pi_c = 1$. El Lagrangiano es:\n",
    "\n",
    "$$\n",
    "L(\\theta, \\lambda) = f(\\theta) - \\lambda \\left(\\sum_{c=1}^{k} \\pi_c - 1\\right).\n",
    "$$\n",
    "\n",
    "Las derivadas respecto a $\\pi_c$ y a $\\lambda$, igualadas a cero, resulta en:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(\\theta, \\lambda)}{\\partial \\pi_c} = \\sum_{i=1}^{N} q(t_i=c) \\frac{1}{\\pi_c} - \\lambda & = 0 \\\\\n",
    "\\Leftrightarrow \\pi_c & = \\frac{\\sum_{i=1}^{N} q(t_i=c)}{\\lambda}.\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(\\theta, \\lambda)}{\\partial \\lambda} = \\sum_{c=1}^{k} \\pi_c - 1 & = 0 \\\\\n",
    "\\Leftrightarrow \\sum_{c=1}^{k} \\frac{\\sum_{i=1}^{N} q(t_i=c)}{\\lambda} - 1 & = 0 \\\\\n",
    "\\Leftrightarrow \\sum_{i=1}^{N} \\underbrace{\\sum_{c=1}^{k} q(t_i=c)}_{1} - \\lambda & = 0 \\\\\n",
    "\\Leftrightarrow \\lambda &= N.\n",
    "\\end{align}\n",
    "\n",
    "Finalmente:\n",
    "\n",
    "$$\n",
    "\\pi_c = \\frac{\\sum_{i=1}^{N} q(t_i=c)}{N}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En resumen:**\n",
    "\n",
    "- $\\mu_c = \\frac{\\sum_{i=1}^{N} q(t_i=c)x_i}{\\sum_{i=1}^{N} q(t_i=c)}$.\n",
    "- $\\sigma_c^2 = \\frac{\\sum_{i=1}^{N} q(t_i=c)(x_i - \\mu_c)^2}{\\sum_{i=1}^{N} q(t_i=c)}$.\n",
    "- $\\pi_c = \\frac{\\sum_{i=1}^{N} q(t_i=c)}{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Nota:\n",
    "\n",
    "Acá hemos resuelto el caso escalar para facilitar el entendimiento, sin embargo, también es posible obtener soluciones cerradas en el caso multivariable.\n",
    "\n",
    "En el caso multivariable ($x \\in \\mathbb{R}^n$), la función $f(\\theta)$ es:\n",
    "\n",
    "$$\n",
    "f(\\theta) = \\sum_{i=1}^{N} \\sum_{c=1}^{k} q(t_i=c) \\left[\\log \\pi_c - \\frac{1}{2}\\log (2 \\pi)^n + \\frac{1}{2}\\log \\det(\\Lambda_c) - \\frac{1}{2}(x_i - \\mu_c)^T \\Lambda_c (x_i - \\mu_c)\\right],\n",
    "$$\n",
    "\n",
    "donde $\\Lambda_c = \\Sigma_c^{-1}$.\n",
    "\n",
    "En este caso, es útil saber los siguientes gradientes:\n",
    "\n",
    "1. Forma cuadrática respecto al vector ($y \\in \\mathbb{R}^n$, $Q = Q^T \\in\\mathbb{R}^{n \\times n}$):\n",
    "   $$\n",
    "   \\frac{\\partial y^T Q y}{\\partial y} = 2 Q y.\n",
    "   $$\n",
    "   \n",
    "2. Forma cuadrática respecto a la matriz ($y \\in \\mathbb{R}^n$, $Q = Q^T \\in\\mathbb{R}^{n \\times n}$):\n",
    "   $$\n",
    "   \\frac{\\partial y^T Q y}{\\partial Q} = y y^T.\n",
    "   $$\n",
    "   \n",
    "3. Gradiente del log-det ($Q = Q^T \\in\\mathbb{R}^{n \\times n}$):\n",
    "   $$\n",
    "   \\frac{\\partial \\log \\det (Q)}{\\partial Q} = Q^{-1}.\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo que se obtienen los siguientes resultados (análogos al caso escalar):\n",
    "\n",
    "$$\n",
    "\\mu_c = \\frac{\\sum_{i=1}^{N} q(t_i=c)x_i}{\\sum_{i=1}^{N} q(t_i=c)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_c = \\frac{\\sum_{i=1}^{N} q(t_i=c)(x_i - \\mu_c) (x_i - \\mu_c)^T}{\\sum_{i=1}^{N} q(t_i=c)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_c = \\frac{\\sum_{i=1}^{N} q(t_i=c)}{N}.\n",
    "$$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. En resumen\n",
    "\n",
    "**Entrenamiento de parámetros para GMM:**\n",
    "\n",
    "1. Inicializar los parámetros:\n",
    "   $$\n",
    "   \\theta^0 = \\left\\{(\\pi_c^0, \\mu_c^0, \\Sigma_c^0): c=1,\\dots, k \\right\\}.\n",
    "   $$\n",
    "   \n",
    "   - $\\pi_c^0$ se puede inicializar con una muestra de la distribución de Dirichlet.\n",
    "   - $\\mu_c^0$ se puede inicializar eligiendo aleatoriamente datos de la muestra para entrenar.\n",
    "   - $\\Sigma_c^0$ se puede inicializar con la matriz identidad, o como $R^T R$ donde $R$ es una matriz de números aleatorios.\n",
    "   \n",
    "2. Repetir hasta la convergencia ($j=0, 1, \\dots$):\n",
    "\n",
    "   - E-step: para todo $c=1,\\dots,k$, para todo $i=1,\\dots,N$,\n",
    "     $$\n",
    "     q^{j+1}(t_i = c) = \\frac{\\mathcal{N}(x_i | \\mu_c^j, \\Sigma_c^j) \\pi_c^j}{\\sum_{c=1}^{k} \\pi_c^j \\mathcal{N}(x | \\mu_c^j, \\Sigma_c^j)}\n",
    "     $$\n",
    "     \n",
    "   - M-step: para todo $c=1,\\dots,k$\n",
    "     $$\n",
    "     \\mu_c^{j+1} = \\frac{\\sum_{i=1}^{N} q^{j+1}(t_i=c)x_i}{\\sum_{i=1}^{N} q^{j+1}(t_i=c)},\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "     \\Sigma_c^{j+1} = \\frac{\\sum_{i=1}^{N} q^{j+1}(t_i=c)(x_i - \\mu_c) (x_i - \\mu_c)^T}{\\sum_{i=1}^{N} q^{j+1}(t_i=c)},\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "     \\pi_c^{j+1} = \\frac{\\sum_{i=1}^{N} q^{j+1}(t_i=c)}{N}.\n",
    "     $$\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sobre el ejemplo de clientes bancarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos sklearn.model_selection.train_test_split\n",
    "\n",
    "# Importamos sklearn.mixture.GaussianMixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianMixture model help\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de mezclas Gaussianas\n",
    "\n",
    "\n",
    "# Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros óptimos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussianas ajustadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n",
    "plt.scatter(data[\"income\"], data[\"debt\"], c=\"gray\", alpha=0.6)\n",
    "\n",
    "# Gaussiana 1\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.linspace(0, 8, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X1.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "\n",
    "# Gaussiana 2\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.linspace(0, 8, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X2.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "\n",
    "# Gaussiana 3\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.linspace(0, 8, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X3.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "\n",
    "plt.xlabel(\"Ingresos mensuales (x100k MXN)\")\n",
    "plt.ylabel(\"Deuda (x100k MXN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por colores - respecto a la probabilidad de pertenencia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las razones principales para considerar clustering probabilístico era la elección del número de clusters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x | \\theta) = \\sum_{c=1}^{k} \\pi_c \\mathcal{N}(x | \\mu_c, \\Sigma_c).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_gmm(X, mu, sigma, pi):\n",
    "    \"\"\"\n",
    "    Log-likelihood of the data wrt Gaussian Mixture Model.\n",
    "    :param data: Data.\n",
    "    :param mu: Means of the components of the GMM.\n",
    "    :param sigma: Covariances of the components of the GMM.\n",
    "    :param pi: Weights of the components of the GMM.\n",
    "    :return: Log-likelihood of the data wrt GMM.\n",
    "    \"\"\"\n",
    "    # Number of clusters\n",
    "    k = mu.shape[0]\n",
    "    # Number of points\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Individual likelihood of each point to each normal\n",
    "    ind_likelihood = np.zeros((N, k))\n",
    "    for j in range(k):\n",
    "        ind_likelihood[:, j] = multivariate_normal.pdf(X, mean=mu[j, :], cov=sigma[j, :, :])\n",
    "    \n",
    "    # Log likelihood\n",
    "    log_likelihood = np.log(ind_likelihood.dot(pi)).mean()\n",
    "        \n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de datos\n",
    "\n",
    "log_likelihood_train = []\n",
    "log_likelihood_test = []\n",
    "for k in range(2, 11):\n",
    "    # Instanciamos el algoritmo\n",
    "    \n",
    "    \n",
    "    # Entrenamos\n",
    "    \n",
    "    \n",
    "    # Métrica con datos de entrenamiento\n",
    "    \n",
    "    \n",
    "    # Métrica con datos de prueba\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera, de acuerdo a la log-verosimilitud sobre el conjunto de pruebas, el número de componentes óptimo es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de componentes óptimo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¡No hay overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
