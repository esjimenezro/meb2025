{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminares Matemáticos del Algoritmo de Maximización de la Esperanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/3/3d/EM_Process.jpg\" width=\"500px\" height=\"300px\" />\n",
    "\n",
    "> En esta serie de cuadernos estaremos estudiando el algoritmo de maximización de la esperanza que introdujimos en la clase pasada en el contexto de mezclas Gaussianas.\n",
    "\n",
    "> Este interesante algoritmo sirve para entrenar los parámetros de una gran mayoría de los modelos con variables latentes, incluido el modelo de mezclas Gaussianas.\n",
    "\n",
    "> Sin embargo, para entenderlo es necesario tener claros varios conceptos matemáticos preliminares.\n",
    "\n",
    "> **Objetivos:**\n",
    "> - Explicar la desigualdad de Jensen para funciones cóncavas.\n",
    "> - Comprender la divergencia de Kullback-Leibler como una medida de proximidad de distribuciones.\n",
    "> - Probar propiedades sencillas relativas a la divergencia de Kullback-Leibler.\n",
    "\n",
    "> **Referencias:**\n",
    "> - Bayesian Methods for Machine Learning course, HSE University, Coursera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Funciones cóncavas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Definición.* Sea $f:\\Omega \\subseteq \\mathbb{R}^{k} \\to \\mathbb{R}$ una función real. Decimos que $f$ es cóncava si para todo $x_1, x_2 \\in \\Omega$ y $\\alpha \\in (0, 1)$\n",
    ">\n",
    "> $$\n",
    "  f(\\alpha x_1 + (1 - \\alpha) x_2) \\geq \\alpha f(x_1) + (1 - \\alpha) f(x_2).\n",
    "  $$\n",
    "  \n",
    "  \n",
    "La definición anterior lo que dice es que una función es cóncava si para cualesquier par de puntos sobre la gráfica de la función $f$, la función supera al segmento de recta que une esos dos puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veámoslo a través de un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos numpy\n",
    "\n",
    "# Importamos matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función cóncava\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector de x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué ejemplos de funciones cóncavas se les vienen a la mente?**\n",
    "\n",
    "<details>\n",
    "  <summary>Descubrir</summary>\n",
    "- $f(x) = \\log_a(x)$\n",
    "- $f(x) = -\\frac{1}{x}; \\mathrm{Dom}[f]=\\mathbb{R}_+$\n",
    "- $f(x) = x^a; \\mathrm{Dom}[f]=\\mathbb{R}_+, \\forall a \\in (0, 1)$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguramente recordarán de cálculo un resultado para funciones dos veces diferenciables:\n",
    "\n",
    "> *Teorema.* Sea $f: \\mathbb{R} \\to \\mathbb{R}$ una función dos veces diferenciable en un intervalo abierto $(a, b)$. Entonces, $f$ es cóncava si y sólo si $f''(x) \\leq 0$ para todo $x \\in (a, b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar el teorema para comprobar la concavidad de las funciones:\n",
    "\n",
    "<details>\n",
    "  <summary>Descubrir</summary>\n",
    "- $f(x) = \\log_a(x)$\n",
    "- $f(x) = -\\frac{1}{x}; \\mathrm{Dom}[f]=\\mathbb{R}_+$\n",
    "- $f(x) = x^a; \\mathrm{Dom}[f]=\\mathbb{R}_+, \\forall a \\in (0, 1)$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Desigualdad de Jensen\n",
    "\n",
    "Ahora, la definición de concavidad sólo involucra un par de puntos. Sin embargo, es posible extender esto para cualquier cantidad de puntos:\n",
    "\n",
    "> *Proposición.* Sea $f:\\Omega \\subseteq \\mathbb{R}^{k} \\to \\mathbb{R}$ una función cóncava. Entonces para cualquier selección de números $\\alpha_i \\geq 0$, con $i = 1, \\dots, m$, tales que $\\sum_{i=1}^m \\alpha_i = 1$ y cualquier selección de elementos $x_i \\in \\Omega$, con $i = 1, \\dots, m$, se tiene que:\n",
    "> \n",
    "> $$\n",
    "  f\\left(\\sum_{i=1}^{m} \\alpha_i x_i\\right) \\geq \\sum_{i=1}^m \\alpha_i f(x_i)\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿La propiedad que deben cumplir las $\\alpha_i$ se les hace conocida?**\n",
    "\n",
    "- $\\alpha_i \\geq 0$, con $i = 1, \\dots, m$\n",
    "- $\\sum_{i=1}^m \\alpha_i = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En términos probabilísticos, esto se podría escribir como:\n",
    "\n",
    "> *Proposición.* Sea $f:\\Omega \\subseteq \\mathbb{R}^{k} \\to \\mathbb{R}$ una función cóncava y $X$ una variable aleatoria (multivariable de dimensión $k$). Entonces\n",
    "> \n",
    "> $$\n",
    "  f(E[X]) \\geq E[f(X)].\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio.** Sea $X \\sim \\mathcal{N}(0, 1)$. Definimos la VA $Y$ como una función determinista de $X$, $Y=X^2 + 5$. ¿Cuál de las siguientes afirmaciones es cierta?\n",
    "\n",
    "- La desigualdad de Jensen no se puede aplicar puesto que $\\log (x^2 + 5)$ no es cóncava.\n",
    "- $E[\\log (x^2 + 5)] \\geq \\log E[x^2 + 5]$\n",
    "- $E[\\log (x^2 + 5)] \\leq \\log E[x^2 + 5]$\n",
    "- $E[\\log y] \\geq \\log E[y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Divergencia de Kullback-Leibler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comúnmente, no solo en el contexto del algoritmo de maximización de la esperanza, necesitaremos medir la diferencia (o similaridad) entre dos distribuciones de probabilidad.\n",
    "\n",
    "Una manera de medir esto es usando la **divergencia de Kullback-Leibler**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo.** Supongamos que tenemos dos Gaussianas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos scipy.stats.norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos dos VA normales N(0,1) y N(1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos ambas densidades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una posible manera de medir la diferencia entre estas distribuciones sería medir la \"distancia\" entre los parámetros, la cual en este caso es 1.\n",
    "\n",
    "Sin embargo este enfoque tiene dos problemas:\n",
    "\n",
    "1. ¿Qué pasa si las distribuciones que quiero comparar son de distintas familias?\n",
    "\n",
    "2. Aún cuando las distribuciones son de la misma familia, veamos el siguiente caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos dos VA normales N(0, 10) y N(1, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos sus densidades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, aplicando el mismo principio, la distancia también sería 1. Sin embargo, podemos apreciar que estas distribuciones se parecen muchísimo más que las anteriores. Por esto deberíamos considerar una medida alternativa de similitud / diferencia entre distribuciones.\n",
    "\n",
    "Esta medida es la **divergencia de Kullback-Leibler**.\n",
    "\n",
    "> *Definición.* Dadas dos distribuciones de probabilidad, la divergencia de Kullback-Leibler se define como:\n",
    ">\n",
    "> $$\n",
    "  \\mathcal{KL}(q || p) = \\int_{-\\infty}^{\\infty} q(x) \\log \\frac{q(x)}{p(x)} d x = E_{q(x)}\\left[\\log \\frac{q(x)}{p(x)}\\right]\n",
    "  $$\n",
    ">  \n",
    "> si las variables son continuas, o\n",
    ">\n",
    "> $$\n",
    "  \\mathcal{KL}(q || p) = \\sum_{x} q(x) \\log \\frac{q(x)}{p(x)} = E_{q(x)}\\left[\\log \\frac{q(x)}{p(x)}\\right]\n",
    "  $$\n",
    ">\n",
    "> si las variables son discretas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos scipy.integrate.quad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer conjunto de distribuciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divergencia KL entre P y Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo conjunto de distribuciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divergencia KL entre P y Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que si evaluamos la diferencia entre las distribuciones en el primer caso (más distintas) obtenemos un valor de divergencia de 0.5, mientras que para el segundo caso (más parecidas) obtenemos un valor de divergencia de 0.005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propiedades de la divergencia de Kullback-Leibler\n",
    "\n",
    "Tenemos las siguientes propiedades:\n",
    "\n",
    "1. $\\mathcal{KL}(q || p) \\neq \\mathcal{KL}(p || q)$.\n",
    "2. $\\mathcal{KL}(q || q) = 0$\n",
    "3. $\\mathcal{KL}(q || p) \\geq 0$\n",
    "\n",
    "*Prueba.* En clase ... (Considerar el negativo de la divergencia y usar la desigualdad de Jensen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarea (opcional - válida por una tarea anterior):**\n",
    "\n",
    "Supongamos que $q(x)=\\mathcal{N}(x | \\mu_1, \\sigma_1^2)$ y $p(x)=\\mathcal{N}(x | \\mu_2, \\sigma_2^2)$.\n",
    "\n",
    "Calcular:\n",
    "\n",
    "1. $\\mathcal{KL}(q || p)$.\n",
    "\n",
    "2. $\\mathcal{KL}(p || q)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
