{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos con variables latentes y repaso del algoritmo K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/e/e5/KMeans-Gaussian-data.svg\" width=\"500px\" height=\"300px\" />\n",
    "\n",
    "> **¿Qué significa variable latente?**\n",
    "\n",
    "> Para responder a esta pregunta, nos remontamos hacia la raiz *latina* etimológica de la palabra latente. Esta palabra viene la palabra en Latín **latens** que significa escondido u oculto.\n",
    "\n",
    "> En el contexto del modelado probabilístico nos referimos con variables latentes a variables que nunca observamos, pero que (inferimos) están ahí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Variables latentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Porqué consideramos variables latentes?\n",
    "\n",
    "Hay diversas razones por las que permitirnos incluir variables latentes en nuestros modelos cobra muchísima importancia. Algunas de ellas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **No porque no observemos una variable significa que no exista.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Muchas veces nos permiten conseguir modelos más simples.**\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "Una empresa acaba de abrir una posición en el equipo de ciencia de datos. En este sentido, el departamento de RH está interesado en entrevistar a varios candidatos para encontrar a alguien idóneo para la posición.\n",
    "\n",
    "Para ello, ya se tiene un tabulador que involucra varias variables:\n",
    "- Grado académico.\n",
    "- Promedio de calificaciones del último grado académico.\n",
    "- Entrevista telefónica.\n",
    "- Entrevista en vivo.\n",
    "\n",
    "Sin embargo, la entrevista en vivo es un evento que puede llegar a involucrar muchos recursos económicos, y por experiencia, hay varios candidatos que se pueden descartar con solo el conocimiento de las otras variables. La idea es desarrollar un modelo usando datos históricos del departamento de RH:\n",
    "\n",
    "| Candidato | Grado | Promedio | E. Telefónica | E. Vivo |\n",
    "| --------- | ----- | -------- | ------------- | ------- |\n",
    "| 1         | Lic   | 8.4      | 7             | 5       |\n",
    "| 2         | Maes  | 8.0      | 7             | 6       |\n",
    "| 3         | Lic   | 9.5      | 8             | 9       |\n",
    "| 4         | Doc   | 8.9      | 9             | 10      |\n",
    "\n",
    "Si intentamos establecer un modelo que relacione estas variables, después de revisarlo un poco, llegaríamos a que todas estas variables están relacionadas entre sí, obteniendo un modelo completamente conectado,\n",
    "\n",
    "![fully_conn](figures/fully_connected.png)\n",
    "\n",
    "con lo que no tendríamos ni una sola pista de la estructura del modelo, y tendríamos que definir la probabilidad conjunta sobre todas las variables (número exponencial de parámetros)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Alternativa 1*: considerar un modelo estructurado del tipo\n",
    "\n",
    "$$\n",
    "p(x_1, x_2, x_3, x_4) = \\frac{\\exp\\{-w^T x\\}}{Z},\n",
    "$$\n",
    "\n",
    "con lo que solo tendríamos 5 parámetros $w_0, w_1, w_2, w_3, w_4$. Sin embargo, $Z$ es una constante de normalización que involucra una suma sobre todos los posibles valores de las cuatro variables aleatorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Alternativa 2*: considerar una variable latente de Inteligencia\n",
    "\n",
    "![latent](figures/latent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con lo que el modelo sería:\n",
    "\n",
    "$$\n",
    "p(x_1, x_2, x_3, x_4) = \\sum_{I} p(x_1, x_2, x_3, x_4 | I) p(I) = \\sum_{I} p(x_1 | I)p(x_2 | I)p(x_3 | I)p(x_4 | I) p(I)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto reducimos notablemente la complejidad del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Aplicaciones prácticas -> Clustering -> Segmentación de clientes, Motores de búsqueda, Sistemas de recomendación, ...**\n",
    "\n",
    "En aplicaciones clustering pretendemos descubrir segmentaciones en los datos. Esta segmentación la podemos entender como una variable latente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probablemente ya hayan escuchado hablar de clustering. Como tal, es de las aplicaciones más importantes en aprendizaje **no supervisado**, y seguramente en un proyecto de análisis de datos no va a pasar más de un mes para cuando necesiten utilizar este tipo de técnicas. Así que vamos a estudiar un par de ellas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**\n",
    "\n",
    "Los departamentos de créditos en general (personas, pymes, empresarial) normalmente estudian la relación entre ingresos y deuda del candidato a crédito para decidir si acreditan o no a la persona. \n",
    "\n",
    "Hay varias heurísticas que se utilizan. Por ejemplo:\n",
    "\n",
    "- Si las deudas rebasan el 40% del apalancamiento (capital social + ingresos + pasivos), es una empresa de alto riesgo.\n",
    "- Si las deudas rebasan 3 meses de ingresos, es una empresa de alto riesgo.\n",
    "\n",
    "Se puede estudiar esta relación, y segmentar a los clientes de acuerdo a su perfil en estas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos función para generar datos\n",
    "\n",
    "# Importamos pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que queremos en clustering es identificar los grupos a los que pertenecen cada uno de los clientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grupos \"reales\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta idea se conoce como **Hard clustering**; bajo este esquema, identificamos para cada punto un único grupo al que pertenece, es decir:\n",
    "\n",
    "$$\n",
    "\\text{cluster-id}_x = f(x)\n",
    "$$\n",
    "\n",
    "- Los puntos verdes son 100% verdes.\n",
    "- Los puntos azules son 100% azules.\n",
    "- Los puntos grises son 100% grises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, fijemos un momento nuestra atención en los recuadros a continuación ($[2, 3] \\times [4, 5]$ y $[4, 5] \\times [3, 4]$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(data[\"income\"], data[\"debt\"], c=data[\"labels\"], cmap=\"Accent\", alpha=0.6)\n",
    "plt.axis([1, 3, 4.5, 5.5])\n",
    "plt.xlabel(\"Ingresos mensuales (x100k MXN)\")\n",
    "plt.ylabel(\"Deuda (x100k MXN)\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(data[\"income\"], data[\"debt\"], c=data[\"labels\"], cmap=\"Accent\", alpha=0.6)\n",
    "plt.axis([3.5, 5, 2, 4])\n",
    "plt.xlabel(\"Ingresos mensuales (x100k MXN)\")\n",
    "plt.ylabel(\"Deuda (x100k MXN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde un punto de vista intuitivo, los puntos en estos sectores no es muy claro a qué grupo pertenecen. Estaríamos tentados a decir que pertenecen a un grupo con cierta probabilidad y con cierta probabilidad a otro.\n",
    "\n",
    "Esta idea se conoce como **Soft clustering**, y está estrechamente relacionado a **clustering probabilístico**.\n",
    "\n",
    "$$\n",
    "p(\\text{cluster-id}_x |x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al darle un enfoque probabilístico, tenemos varias ventajas colaterales:\n",
    "\n",
    "- Sintonización de hiperparámetros.\n",
    "- Modelo generativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Un algoritmo de hard clustering: K-Means\n",
    "\n",
    "Aunque el K-Means es uno de los algoritmos de hard clustering más conocidos y usados, lo veremos en un par de sesiones más desde una perspectiva probabilística. \n",
    "\n",
    "De manera que nos conviene estudiarlo antes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema:** dado un conjunto de observaciones $x_1, x_2, \\dots, x_N \\in \\mathbb{R}^d$, se debe particionar las $N$ observaciones en $k$ ($\\leq N$) clusters $\\{1, 2, \\dots, k\\}$, de manera que se minimice la suma de distancias al cuadrado (varianza)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algoritmo:**\n",
    "\n",
    "1. Inicializar los parámetros $\\theta = \\{\\mu_1, \\dots, \\mu_k\\}$ de manera aleatoria.\n",
    "2. Repetir hasta la convergencia (hasta que los parámetros no varíen):\n",
    "   1. Para cada punto calcule el centroide más cercano:\n",
    "      $$\n",
    "      c_i = \\arg \\min_{c} ||x_i - \\mu_c||.\n",
    "      $$\n",
    "      \n",
    "   2. Actualizar centroides:\n",
    "      $$\n",
    "      \\mu_c = \\frac{\\sum_{i: c_i = c} x_i}{\\sum_{i: c_i = c} 1}\n",
    "      $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarea:** Programar el algoritmo K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosotros usaremos sklearn durante la clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos sklearn.cluster.KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo de sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos el algoritmo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroides de los clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo se ve bien hasta acá. **¿Qué pasa si aumentamos el número de clusters?**\n",
    "\n",
    "Una métrica que podemos usar para ver qué tan bueno está siendo el agrupado es la suma de las distancias al cuadrado de cada punto a su centroide respectivo:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^N ||x_i - \\mu_{c_i}||^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos train_test_split\n",
    "\n",
    "# Importamos numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msd(X, cluster_id, centroids):\n",
    "    \"\"\"\n",
    "    Mean squared distance.\n",
    "    :param data: Data.\n",
    "    :param centroids: Centroids.\n",
    "    :return: Mean squared distance.\n",
    "    \"\"\"\n",
    "    # Number of clusters\n",
    "    k = centroids.shape[0]\n",
    "    # Number of points\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Distances initialization\n",
    "    distances = np.zeros(N)\n",
    "    \n",
    "    # Compute distances to corresponding cluster\n",
    "    for j in range(k):\n",
    "        distances[cluster_id == j] = np.linalg.norm(X[cluster_id == j] - centroids[j, :], axis=1)\n",
    "        \n",
    "    return (distances**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data[[\"income\", \"debt\"]], test_size=0.2)\n",
    "\n",
    "msd_train = []\n",
    "msd_test = []\n",
    "for k in range(2, 20):\n",
    "    # Instanciamos el algoritmo\n",
    "        \n",
    "    # Entrenamos\n",
    "        \n",
    "    # Métrica con datos de entrenamiento\n",
    "        \n",
    "    # Métrica con datos de prueba\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos de MSD en train y test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico - final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como observamos, esta métrica siempre decrece con la cantidad de clusters, lo que hace bastante complejo elegir un número de clusters adecuado cuando este es desconocido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelo de mezcla Gaussiana (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos, el K-Means (y en general los algoritmos de hard clustering) tienen varios detalles:\n",
    "\n",
    "- No es claro cómo elegir el número de clusters.\n",
    "- Hay puntos que podrían estar en una frontera entre dos o más clusters, y el hard clustering no nos permite tener incertidumbre en la pertenencia.\n",
    "\n",
    "Para lidiar con estos problemas, podemos plantear un modelo probablístico de nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora, conocemos algunas distribuciones. Entre ellas la Gaussiana, para la cual ya sabemos como estimar sus parámetros.\n",
    "\n",
    "¿Qué pasa si intentamos ajustar una **distribución Gaussiana** a los datos? Es decir, si modelamos los datos con\n",
    "\n",
    "$$\n",
    "p(x|\\theta) = \\mathcal{N}(x|\\mu, \\Sigma), \\qquad \\theta=\\{\\mu, \\Sigma\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos parámetros - media y covarianza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos multivariate_normal from scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos VA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n",
    "\n",
    "\n",
    "# Gaussiana\n",
    "x = np.linspace(0, 8, 100)\n",
    "y = np.linspace(0, 9, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "plt.xlabel(\"Ingresos mensuales (x100k MXN)\")\n",
    "plt.ylabel(\"Deuda (x100k MXN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, este modelo no parece corresponder con nuestros datos. La región de máxima probabilidad (media) cae en un punto medio entre los clusters, y allí no hay muchos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué pasa si usamos varias Gaussianas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos parámetros - medias y covarianzas\n",
    "\n",
    "\n",
    "# Definimos VA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n",
    "plt.scatter(data[\"income\"], data[\"debt\"], c=\"gray\", alpha=0.6)\n",
    "\n",
    "# Gaussiana 1\n",
    "x = np.linspace(0, 6, 100)\n",
    "y = np.linspace(0, 5, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X1.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "\n",
    "# Gaussiana 2\n",
    "x = np.linspace(0, 4, 100)\n",
    "y = np.linspace(4, 9, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X2.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "\n",
    "# Gaussiana 3\n",
    "x = np.linspace(3, 8, 100)\n",
    "y = np.linspace(2, 8, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X3.pdf(np.dstack([x, y]))\n",
    "plt.contour(x, y, z)\n",
    "\n",
    "plt.xlabel(\"Ingresos mensuales (x100k MXN)\")\n",
    "plt.ylabel(\"Deuda (x100k MXN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¡Mucho mejor!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada Gaussiana explica un cluster de puntos, y el modelo general sería una suma ponderada de estas densidades Gaussianas:\n",
    "\n",
    "$$\n",
    "p(x | \\theta) = \\sum_{c=1}^{3} \\pi_c \\mathcal{N}(x | \\mu_c, \\Sigma_c), \\qquad \\theta = \\{\\pi_1, \\pi_2, \\pi_3, \\mu_1, \\mu_2, \\mu_3, \\Sigma_1, \\Sigma_2, \\Sigma_3\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Y esto cómo lo interpretamos?**\n",
    "\n",
    "Bueno, pues si logramos encontrar los parámetros $\\pi_1, \\pi_2, \\pi_3, \\mu_1, \\mu_2, \\mu_3, \\Sigma_1, \\Sigma_2, \\Sigma_3$ para este conjunto de datos, habremos resuelto el problema de (soft) clustering, ya que encontraremos para cada punto la probabilidad de que venga de cada una de las Gaussianas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué ventajas tenemos?**\n",
    "\n",
    "Como ventaja respecto a usar una sola Gaussiana, hemos añadido flexibilidad a nuestro modelo. Es decir, con esta estuctura podemos representar conjuntos de datos complejos.\n",
    "\n",
    "> En efecto, podemos aproximar casi cualquier distribución continua con una mezcla de Gaussianas con precisión arbitraria, dado que incluyamos un número suficiente de Gaussianas en la mezcla.\n",
    "\n",
    "**¿A qué costo?**\n",
    "\n",
    "La cantidad de parámetros que debemos estimar se multiplica por la cantidad de Gaussianas en la mezcla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo encontramos (entrenamos) los parámetros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos maximizar la función de verosimilitud (suposición de independencia):\n",
    "\n",
    "$$\n",
    "\\max_{\\theta} p(X | \\theta) = \\prod_{i=1}^N p(x_i | \\theta) = \\prod_{i=1}^N \\sum_{c=1}^{3} \\pi_c \\mathcal{N}(x_i | \\mu_c, \\Sigma_c)\n",
    "$$\n",
    "\n",
    "sujeto a:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{c=1}^3 \\pi_c & = 1 \\\\\n",
    "\\pi_c & \\geq 0 \\quad \\text{for } c=1,2,3\\\\\n",
    "\\Sigma_c & \\succ 0 \\quad \\text{for } c=1,2,3\n",
    "\\end{align}\n",
    "\n",
    "Es decir, las matrices de covarianza deben ser definidas positivas (¿por qué?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complejidades numéricas:**\n",
    "\n",
    "Este problema de optimización puede ser resuelto numéricamente con un algoritmo como el gradiente descendiente. Sin embargo,\n",
    "\n",
    "1. La restricción sobre la matriz de covarianzas hace el problema de optimización muy complejo de resolver numéricamente hablando.\n",
    "\n",
    "   Una simplificación para poder trabajar con esta restricción es suponer que las matrices de covarianza son diagonales:\n",
    "\n",
    "$$\n",
    "\\Sigma_c = \\text{diag}(\\sigma_{c1}, \\sigma_{c2}, \\dots, \\sigma_{cn}),\n",
    "$$\n",
    "\n",
    "2. La suma dentro del producto también hace bastante complejo el cálculo de los gradientes. Comúnmente, para evitar el producto se toma logaritmo de la verosimilitud:\n",
    "\n",
    "   $$\n",
    "   \\log p(X | \\theta) = \\log \\left(\\prod_{i=1}^N p(x_i | \\theta) \\right)= \\sum_{i=1}^N \\log\\left(\\sum_{c=1}^{3} \\pi_c \\mathcal{N}(x_i | \\mu_c, \\Sigma_c)\\right)\n",
    "   $$\n",
    "\n",
    "   y con esto, podemos observar que permanece una suma ponderada dentro del logaritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Y entonces?\n",
    "\n",
    "Afortunadamente, existe un algoritmo alternativo con base probabilística llamado **algoritmo de maximización de la esperanza**, el cual estaremos estudiando en las próximas clases no solo para el problema de mezclas Gaussianas, sino para entrenar cualquier modelo con **variables latentes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Variables latentes?**\n",
    "\n",
    "Recordamos que propusimos el siguiente modelo:\n",
    "\n",
    "$$\n",
    "p(x | \\theta) = \\sum_{c=1}^{3} \\pi_c \\mathcal{N}(x | \\mu_c, \\Sigma_c), \\qquad \\theta = \\{\\pi_1, \\pi_2, \\pi_3, \\mu_1, \\mu_2, \\mu_3, \\Sigma_1, \\Sigma_2, \\Sigma_3\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo en realidad, lo podemos pensar como un modelo con una variable latente $t$ que determina a cuál Gaussiana pertenece cada punto:\n",
    "\n",
    "![gmm](figures/gmm.png)\n",
    "\n",
    "Entonces, razonablemente podemos atribuirle a $t$ tres posibles valores (1, 2, y 3), que nos dicen de qué Gaussiana viene el punto. Recordamos que $t$ es una variable latente, nunca la observamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, razonando probabilísticamente, después de entrenar nuestra mezcla Gaussiana, podríamos preguntarle al modelo, ¿Cuál es el valor más probable de $t$ dado el punto $x$? --> **Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este modelo, podemos asignar las siguientes probabilidades:\n",
    "\n",
    "- Previa:\n",
    "  \n",
    "  $$\n",
    "  p(t=c | \\theta) = \\pi_c\n",
    "  $$\n",
    "  \n",
    "- Verosimilitud:\n",
    "  \n",
    "  $$\n",
    "  p(x | t=c, \\theta) = \\mathcal{N}(x | \\mu_c, \\Sigma_c)\n",
    "  $$\n",
    " \n",
    "Razonable, ¿no?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y con lo anterior,\n",
    "\n",
    "$$\n",
    "p(x | \\theta) = \\sum_{c=1}^3 p(x, t=c | \\theta) = \\sum_{c=1}^3 \\underbrace{p(x | t=c, \\theta)}_{\\mathcal{N}(x | \\mu_c, \\Sigma_c)} \\underbrace{p(t=c | \\theta)}_{\\pi_c},\n",
    "$$\n",
    "\n",
    "justo como el modelo intuitivo que habíamos propuesto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de maximización de la esperanza para mezclas Gaussianas - Intuición\n",
    "\n",
    "Supongamos que tenemos los siguientes puntos de tamaños de playeras, y queremos definir cuáles son talla chica y cuales son talla grande:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shirts_size_data import generate_shirts_data\n",
    "from scipy.stats.distributions import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shirts_data = generate_shirts_data()\n",
    "shirts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(shirts_data[\"size\"], np.zeros(len(shirts_data)), c=\"gray\", alpha=0.6)\n",
    "plt.ylim([-0.01, 0.3])\n",
    "plt.xlabel(\"Tamaño (cm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo estimamos los parámetros de nuestro modelo de variable latente?\n",
    "\n",
    "Analicemos varios casos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Si de entrada supiéramos cuáles playeras son chicas y cuáles grandes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(shirts_data.loc[shirts_data[\"labels\"] == 0, \"size\"],\n",
    "            np.zeros((shirts_data[\"labels\"] == 0).sum()),\n",
    "            alpha=0.6)\n",
    "plt.scatter(shirts_data.loc[shirts_data[\"labels\"] == 1, \"size\"],\n",
    "            np.zeros((shirts_data[\"labels\"] == 1).sum()),\n",
    "            alpha=0.6)\n",
    "x = shirts_data[\"size\"].copy().values\n",
    "x.sort()\n",
    "mu1 = shirts_data.loc[shirts_data[\"labels\"] == 0, \"size\"].mean()\n",
    "mu2 = shirts_data.loc[shirts_data[\"labels\"] == 1, \"size\"].mean()\n",
    "s1 = shirts_data.loc[shirts_data[\"labels\"] == 0, \"size\"].std()\n",
    "s2 = shirts_data.loc[shirts_data[\"labels\"] == 1, \"size\"].std()\n",
    "plt.plot(x, norm.pdf(x, loc=mu1, scale=s1))\n",
    "plt.plot(x, norm.pdf(x, loc=mu2, scale=s2))\n",
    "plt.ylim([-0.01, 0.3])\n",
    "plt.xlabel(\"Tamaño (cm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(x | t=1, \\theta) = \\mathcal{N}(x | \\mu_1, \\sigma_1)\n",
    "$$\n",
    "\n",
    "Con lo cual:\n",
    "\n",
    "$$\n",
    "\\mu_1 = \\frac{\\sum_{i: t_i = 1} x_i}{\\sum_{i: t_i = 1} 1}, \\qquad \\sigma_1^2 = \\frac{\\sum_{i: t_i = 1} (x_i - \\mu_1)^2}{\\sum_{i: t_i = 1} 1},\n",
    "$$\n",
    "\n",
    "y\n",
    "\n",
    "$$\n",
    "\\mu_2 = \\frac{\\sum_{i: t_i = 2} x_i}{\\sum_{i: t_i = 2} 1}, \\qquad \\sigma_2^2 = \\frac{\\sum_{i: t_i = 2} (x_i - \\mu_1)^2}{\\sum_{i: t_2 = 1} 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Como sablemos, en el algoritmo de mezclas Gaussianas nunca sabremos si un punto pertenece a cierto cluster o no, sino que conoceremos las probabilidades de que pertenezca a cada cluster.\n",
    "\n",
    "   De modo que si conocemos la posterior $p(t | x, \\theta)$, entonces ponderamos lo anterior por esta probabilidad:\n",
    "   \n",
    "   $$\n",
    "\\mu_1 = \\frac{\\sum_{i} p(t_i=1 | x_i, \\theta)x_i}{\\sum_{i} p(t_i=1 | x_i, \\theta)}, \\qquad \\sigma_1^2 = \\frac{\\sum_{i} p(t_i=1 | x_i, \\theta) (x_i - \\mu_1)^2}{\\sum_{i} p(t_i=1 | x_i, \\theta)}.\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ¿Y cómo conocemos la posterior $p(t | x, \\theta)$?\n",
    "\n",
    "   Bueno, pues si conocemos los parámetros, es bastante fácil:\n",
    "   \n",
    "   $$\n",
    "   p(t=c | x, \\theta) = \\frac{p(x | t=c, \\theta) p(t=c | \\theta)}{Z} = \\frac{\\pi_c \\mathcal{N}(x | \\mu_c, \\sigma_c)}{Z}.\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos un razonamiento circular (un problema del tipo, ¿Qué fue primero?, ¿El huevo?, ¿O la gallina?). \n",
    "\n",
    "**¿Cómo lo resolvemos? Iterando...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algoritmo de maximización de la esperanza:**\n",
    "\n",
    "1. Inicializamos los parámetros de cada Gaussiana aleatoriamente.\n",
    "\n",
    "2. Repetir hasta la convergencia:\n",
    "   \n",
    "   - Calcular para cada punto la probabilidad posterior $p(t_i=c | x_i, \\theta)$.\n",
    "   - Actualizar los parámetros de las Gaussianas con las probabildades calculadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
